{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0a23cdbc",
   "metadata": {},
   "source": [
    "# Task 5D\n",
    "\n",
    "| | |\n",
    "|---|---|\n",
    "| Name | Bandi Krishna Chaitanya |\n",
    "| Deakin ID | s225170881 |\n",
    "| Submission Date | |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44c976e4",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Data from https://www.kaggle.com/datasets/mssmartypants/paris-housing-price-prediction is used for this task. The dataset contains information about various properties in Paris, including their prices, locations, and other features. The goal is to predict the price of a property based on its features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c02180b9",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e7a1a08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tabulate import tabulate\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.model_selection import cross_val_score, KFold  \n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.inspection import permutation_importance\n",
    "import shap\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "764ff3b1",
   "metadata": {},
   "source": [
    "## Data Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4349fcd",
   "metadata": {},
   "source": [
    "### Basic Information about the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97fe57f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the dataset\n",
    "paris_real_estate_df = pd.read_csv('ParisHousing.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bd96762",
   "metadata": {},
   "outputs": [],
   "source": [
    "# shape of the dataset\n",
    "print(f\"Shape of the dataset: {paris_real_estate_df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c7c6f28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print all features and their data types using tabulate\n",
    "print(tabulate(paris_real_estate_df.dtypes.reset_index(), headers=['Feature', 'Data Type'], tablefmt='grid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f83589a",
   "metadata": {},
   "source": [
    "All the 17 features in the dataset are numerical. The dataset contains 10,000 rows and 17 columns. \n",
    "\n",
    "There may be some features that are recognized as numerical but are actually categorical. For example, as can be seen from the list of features above, `hasYard`, `hasPool` etc., are binary features that indicate whether a property has a yard, pool, etc. These features should be treated as categorical features rather than numerical features.\n",
    "\n",
    "We shall explore the dataset to identify such features and treat them accordingly.\n",
    "\n",
    "Target feature is `price` and the rest are input features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9877aed",
   "metadata": {},
   "source": [
    "### Check for Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25500e85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the number of missing values in each feature using tabulate\n",
    "missing_values = paris_real_estate_df.isnull().sum().reset_index()\n",
    "missing_values.columns = ['Feature', 'Missing Values']\n",
    "print(tabulate(missing_values, headers='keys', tablefmt='grid'))    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76d67626",
   "metadata": {},
   "source": [
    "The dataset does not contain any missing values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad25c46d",
   "metadata": {},
   "source": [
    "### Univariate Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47ee3058",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the histogram kde plot for each feature using seaborn as a grid\n",
    "plt.figure(figsize=(20, 20))\n",
    "for i, column in enumerate(paris_real_estate_df.columns):\n",
    "    plt.subplot(5, 4, i + 1)\n",
    "    sns.histplot(paris_real_estate_df[column], kde=True)\n",
    "    plt.title(column)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0841e58d",
   "metadata": {},
   "source": [
    "From the histogram plot above, we can see that the following features are binary in nature and should be treated as categorical features:\n",
    "\n",
    "- `hasYard`\n",
    "- `hasPool`\n",
    "- `isNewBuilt`\n",
    "- `hasStormProtector`\n",
    "- `hasStorageRoom`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c384bd0c",
   "metadata": {},
   "source": [
    "And there are some features that appear to be categorical. We shall inspect the number of unique values and their counts to confirm this.\n",
    "\n",
    "- `cityPartRange`\n",
    "- `numPrevOwners`\n",
    "- `hasGuestRoom`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14c54071",
   "metadata": {},
   "outputs": [],
   "source": [
    "# inspect the unique values and their counts for potential categorical features\n",
    "categorical_features = ['cityPartRange', 'numPrevOwners', 'hasGuestRoom']\n",
    "for feature in categorical_features:\n",
    "    unique_counts = paris_real_estate_df[feature].value_counts().reset_index()\n",
    "    unique_counts.columns = ['Value', 'Count']\n",
    "    print(f\"\\nUnique values and counts for {feature}:\")\n",
    "    unique_counts = unique_counts.sort_values(by='Value')\n",
    "    print(tabulate(unique_counts, headers='keys', tablefmt='grid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed900577",
   "metadata": {},
   "source": [
    "All these three features appear to be categorical in nature as they have a limited number of unique values. We shall treat them as categorical features.\n",
    "\n",
    "These are already labeled and do not require any further processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f3080a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# describe the dataset\n",
    "paris_real_estate_description = paris_real_estate_df.describe().transpose()\n",
    "\n",
    "paris_real_estate_description"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c52d5767",
   "metadata": {},
   "source": [
    "### Checking for outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "077f48dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot boxplots for all features to identify outliers\n",
    "plt.figure(figsize=(20, 24))\n",
    "for i, column in enumerate(paris_real_estate_df.columns):\n",
    "    plt.subplot(6, 4, i + 1)\n",
    "    sns.boxplot(x=paris_real_estate_df[column])\n",
    "    plt.title(column)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cd1565d",
   "metadata": {},
   "source": [
    "There don't seem to be any outliers in the dataset, as can be seen from the boxplot above. The features are already in a suitable format for modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea948fed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of unique values in cityCode\n",
    "unique_city_codes = paris_real_estate_df['cityCode'].nunique()\n",
    "print(f\"\\nNumber of unique city codes: {unique_city_codes}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7e9f91f",
   "metadata": {},
   "source": [
    "Based on the descriptive statistics of the dataset, and the histogram plots, we can summarize the features as follows:\n",
    "\n",
    "- There are 17 features in total.\n",
    "- The target feature is `price`.\n",
    "- There are no missing values in the dataset.\n",
    "- Based on the box plots, there are no significant outliers in the dataset.\n",
    "- Some of the features are binary and should be treated as categorical features:\n",
    "  - `hasYard`\n",
    "  - `hasPool`\n",
    "  - `isNewBuilt`\n",
    "  - `hasStormProtector`\n",
    "  - `hasStorageRoom`\n",
    "- Some features appear to be categorical based on the number of unique values:\n",
    "  - `cityPartRange`\n",
    "  - `numPrevOwners`\n",
    "  - `hasGuestRoom` \n",
    "- `cityCode` may not be necessary for the model, as we can use `cityPartRange` to represent the city part.\n",
    "- `made` must be treated as a categorical feature, as it represents the year the property was built and has a limited number of unique values.\n",
    "- `price` does not vary much based on the year the property was built, as per the plots made per city part over the years."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bec48ca8",
   "metadata": {},
   "source": [
    "### Price Analysis over the Years for each City Part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c91161b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for a given cityPartRange, plot the price vs yearBuilt in a grid of plots\n",
    "city_part_ranges = paris_real_estate_df['cityPartRange'].unique()\n",
    "plt.figure(figsize=(20, 20))\n",
    "for i, city_part in enumerate(city_part_ranges):\n",
    "    plt.subplot(5, 4, i + 1)\n",
    "    subset = paris_real_estate_df[paris_real_estate_df['cityPartRange'] == city_part]\n",
    "    sns.scatterplot(x=subset['made'], y=subset['price'], alpha=0.5)\n",
    "    plt.title(f'City Part: {city_part}')\n",
    "    plt.xlabel('Year Built')\n",
    "    plt.ylabel('Price')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e349ea5c",
   "metadata": {},
   "source": [
    "Based on the plots above, it looks like the prices of the houses don't vary much based on the year they were built. They are more or less uniformly distributed across the years."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c23c37f",
   "metadata": {},
   "source": [
    "### Summary of Data Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "722bbba9",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e61b922",
   "metadata": {},
   "source": [
    "### Dropping and Changing Feature Types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "743b59a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the 'cityCode' feature as it has too many unique values\n",
    "paris_real_estate_processing = paris_real_estate_df.drop(columns=['cityCode'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aea5b0fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# binary features that should be treated as categorical\n",
    "binary_features = ['hasYard', 'hasPool', 'isNewBuilt', 'hasStormProtector', 'hasStorageRoom']\n",
    "for feature in binary_features:\n",
    "    paris_real_estate_processing[feature] = paris_real_estate_processing[feature].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a28cef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# other categorical features\n",
    "categorical_features = ['cityPartRange', 'numPrevOwners', 'hasGuestRoom']\n",
    "for feature in categorical_features:\n",
    "    paris_real_estate_processing[feature] = paris_real_estate_processing[feature].astype('category')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7900d8e7",
   "metadata": {},
   "source": [
    "Till now, we did the following preprocessing steps:\n",
    "\n",
    "- Dropped the `cityCode` feature as it has too many unique values.\n",
    "- Converted the binary features to categorical features\n",
    "- Converted the already identified categorical features to categorical data type. These are already labeled and do not require any further processing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4490401",
   "metadata": {},
   "source": [
    "Let us now encode the `made` and `cityPartRange` features as categorical features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e454fd7d",
   "metadata": {},
   "source": [
    "For `made`, since it represents the year the property was built, simply converting it to a categorical type is sufficient. This will allow the model to treat it as a categorical feature without needing further encoding.\n",
    "\n",
    "This is because years have natural ordering and meaningful distance criteria already.\n",
    "\n",
    "For `cityPartRange`, we can use one-hot encoding to represent the different city parts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9512503",
   "metadata": {},
   "source": [
    "### Correlation Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40287c9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check correlation matrix\n",
    "plt.figure(figsize=(20, 20))\n",
    "correlation_matrix = paris_real_estate_processing.corr()\n",
    "sns.heatmap(correlation_matrix, annot=True, fmt='.2f', cmap='coolwarm', square=True, cbar_kws={\"shrink\": .8})\n",
    "plt.title('Correlation Matrix')\n",
    "plt.show()  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d65a6159",
   "metadata": {},
   "source": [
    "There is a PERFECT correlation between `price` and `squareMeters`, which is strange in the real world, but it is what it is in this dataset. This means that the price of a property is directly proportional to its size in square meters.\n",
    "\n",
    "There doesn't seem to be any heavy correlation between the input features, so we can proceed with the preprocessing steps without worrying about multicollinearity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c92fe567",
   "metadata": {},
   "source": [
    "### Splitting the Dataset for Training and Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06b81cb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the processed DataFrame to a CSV file\n",
    "paris_real_estate_processing.to_csv('paris_real_estate_processed.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6461444",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train and test split\n",
    "X = paris_real_estate_processing.drop(columns=['price'])\n",
    "y = paris_real_estate_processing['price']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1a73d10",
   "metadata": {},
   "source": [
    "### One-Hot Encoding of Categorical Feature - `cityPartRange`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3535bac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# categorical encoding for 'made' and one-hot encoding for 'cityPartRange' for training and testing sets\n",
    "X_train = pd.get_dummies(X_train, columns=['cityPartRange'], drop_first=True)\n",
    "X_test = pd.get_dummies(X_test, columns=['cityPartRange'], drop_first=True)\n",
    "\n",
    "# change the feature 'made' to categorical for both training and testing sets\n",
    "X_train['made'] = X_train['made'].astype('category')\n",
    "X_test['made'] = X_test['made'].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8792abe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the train and test sets structure\n",
    "print(\"\\nTraining set structure:\")\n",
    "print(tabulate(X_train.dtypes.reset_index(), headers=['Feature', 'Data Type'], tablefmt='grid'))\n",
    "print(\"\\nTesting set structure:\")\n",
    "print(tabulate(X_test.dtypes.reset_index(), headers=['Feature', 'Data Type'], tablefmt='grid')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55353e99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# shape of the train and test sets\n",
    "print(f\"\\nShape of the training set: {X_train.shape}\")\n",
    "print(f\"Shape of the testing set: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e17ee7dc",
   "metadata": {},
   "source": [
    "### Standardization of Numerical Features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ac4f753",
   "metadata": {},
   "source": [
    "Below features of the dataset need to be standardized:\n",
    "\n",
    "- squareMeters\n",
    "- numberOfRooms\n",
    "- floors\n",
    "- basement\n",
    "- attic\n",
    "- garage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ffc368d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# standardize the features that need to be standardized\n",
    "features_to_standardize = ['squareMeters', 'numberOfRooms', 'floors', 'basement', 'attic', 'garage']\n",
    "scaler = StandardScaler()\n",
    "X_train[features_to_standardize] = scaler.fit_transform(X_train[features_to_standardize])\n",
    "X_test[features_to_standardize] = scaler.transform(X_test[features_to_standardize])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b117013",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write the processed training and testing sets to CSV files\n",
    "X_train.to_csv('X_train_processed.csv', index=False)\n",
    "X_test.to_csv('X_test_processed.csv', index=False)\n",
    "y_train.to_csv('y_train_processed.csv', index=False)\n",
    "y_test.to_csv('y_test_processed.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0da5f6c",
   "metadata": {},
   "source": [
    "## Model Building"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e64bb94d",
   "metadata": {},
   "source": [
    "Since the target feature `price` is a continuous variable, we will use regression models to predict the price of the properties. \n",
    "\n",
    "Based on the instructions, the following metrics will be used to evaluate the models:\n",
    "\n",
    "- Mean Absolute Error (MAE)\n",
    "- Root Mean Squared Error (RMSE)\n",
    "- R-squared (R2)\n",
    "\n",
    "We shall try the following regression models:\n",
    "\n",
    "- Linear Regression\n",
    "- Random Forest Regressor  \n",
    "- Gradient Boosting Regressor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d96983f9",
   "metadata": {},
   "source": [
    "We shall use K-Fold Cross-Validation to evaluate the models. This will help us to get a better estimate of the model's performance on unseen data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ed03956",
   "metadata": {},
   "source": [
    "### Linear Regression with K-Fold Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b683ee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# kf object with 5 splits\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# build a linear regression model with default parameters\n",
    "linear_model = LinearRegression()\n",
    "\n",
    "# do cross-validation\n",
    "linear_scores = cross_val_score(linear_model, X_train, y_train, cv=kf, scoring='neg_mean_squared_error')\n",
    "\n",
    "# get MSE scores\n",
    "linear_mse_scores = -linear_scores  # negate the scores to get positive MSE values\n",
    "\n",
    "# get RMSE scores\n",
    "linear_rmse_scores = linear_mse_scores ** 0.5\n",
    "\n",
    "# get R^2 scores\n",
    "linear_r2_scores = cross_val_score(linear_model, X_train, y_train, cv=kf, scoring='r2')\n",
    "\n",
    "# print all three scores using tabulate\n",
    "linear_scores_table = pd.DataFrame({\n",
    "    'Fold': range(1, len(linear_mse_scores) + 1),\n",
    "    'MSE': linear_mse_scores,\n",
    "    'RMSE': linear_rmse_scores,\n",
    "    'R^2': linear_r2_scores\n",
    "})\n",
    "print(\"\\nLinear Regression Model Cross-Validation Scores:\")\n",
    "print(tabulate(linear_scores_table, headers='keys', tablefmt='grid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "471a9398",
   "metadata": {},
   "source": [
    "As seen above, the R-squared value for the Linear Regression model is 1.0, which indicates that the model is able to explain 100% of the variance in the target variable `price`. \n",
    "\n",
    "This is unusual and generally indicates that the model is overfitting the training data. In real-world scenarios, we would expect the R-squared value to be less than 1.0, indicating that the model is not able to explain all the variance in the target variable.\n",
    "\n",
    "However, since the squareMeters feature is perfectly correlated with the price, the model is probably been able to explain all the variance in the target variable.\n",
    "\n",
    "**Summary of K-Fold Cross-Validation for Linear Regression:**\n",
    "\n",
    "Based on the 5 folds of cross-validation, we see an uniform performance across all the folds."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "243ad10e",
   "metadata": {},
   "source": [
    "### Random Forest Regressor with K-Fold Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f28cd6fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# random forest regressor with default parameters\n",
    "rf_model = RandomForestRegressor(random_state=42)\n",
    "\n",
    "# do cross-validation\n",
    "rf_scores = cross_val_score(rf_model, X_train, y_train, cv=kf, scoring='neg_mean_squared_error')\n",
    "\n",
    "# get MSE scores\n",
    "rf_mse_scores = -rf_scores  # negate the scores to get positive MSE values\n",
    "\n",
    "# get RMSE scores\n",
    "rf_rmse_scores = rf_mse_scores ** 0.5\n",
    "\n",
    "# get R^2 scores\n",
    "rf_r2_scores = cross_val_score(rf_model, X_train, y_train, cv=kf, scoring='r2')\n",
    "\n",
    "# print all three scores using tabulate\n",
    "rf_scores_table = pd.DataFrame({\n",
    "    'Fold': range(1, len(rf_mse_scores) + 1),\n",
    "    'MSE': rf_mse_scores,\n",
    "    'RMSE': rf_rmse_scores,\n",
    "    'R^2': rf_r2_scores\n",
    "})\n",
    "print(\"\\nRandom Forest Regressor Cross-Validation Scores:\")\n",
    "print(tabulate(rf_scores_table, headers='keys', tablefmt='grid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa7eafec",
   "metadata": {},
   "source": [
    "Based on the 5 folds of cross-validation, here are the observations:\n",
    "\n",
    "- The performance of the Random Forest Regressor is consistent across all folds.\n",
    "- We still see a near-perfect R-squared value of 1.0, indicating that the model is able to explain 100% of the variance in the target variable `price`.\n",
    "- Both MSE and RMSE values are higher than those of the Linear Regression model, indicating that the Random Forest Regressor is not performing as well as the Linear Regression model in this case.\n",
    "- This lower performance of Random Forest Regressor indicates that there is a much better linear relationship between the input features and the target feature `price`, which is why the Linear Regression model is performing better. And this is expected as the `squareMeters` feature is perfectly correlated with the `price` feature, which is a linear relationship."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a4d2a68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gradient boosting regressor with default parameters\n",
    "gb_model = GradientBoostingRegressor(random_state=42)\n",
    "\n",
    "# do cross-validation\n",
    "gb_scores = cross_val_score(gb_model, X_train, y_train, cv=kf, scoring='neg_mean_squared_error')\n",
    "\n",
    "# get MSE scores\n",
    "gb_mse_scores = -gb_scores  # negate the scores to get positive MSE values\n",
    "\n",
    "# get RMSE scores\n",
    "gb_rmse_scores = gb_mse_scores ** 0.5\n",
    "\n",
    "# get R^2 scores\n",
    "gb_r2_scores = cross_val_score(gb_model, X_train, y_train, cv=kf, scoring='r2')\n",
    "\n",
    "# print all three scores using tabulate\n",
    "gb_scores_table = pd.DataFrame({\n",
    "    'Fold': range(1, len(gb_mse_scores) + 1),\n",
    "    'MSE': gb_mse_scores,\n",
    "    'RMSE': gb_rmse_scores,\n",
    "    'R^2': gb_r2_scores\n",
    "})\n",
    "print(\"\\nGradient Boosting Regressor Cross-Validation Scores:\")\n",
    "print(tabulate(gb_scores_table, headers='keys', tablefmt='grid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a55541ae",
   "metadata": {},
   "source": [
    "Here are the observations based on the K-Fold Cross-Validation for Gradient Boosting Regressor:\n",
    "\n",
    "- The performance of the Gradient Boosting Regressor is consistent across all folds.\n",
    "- We still see a near-perfect R-squared value of 1.0, indicating that the model is able to explain 100% of the variance in the target variable `price`.\n",
    "- Both MSE and RMSE values are higher than those of the Linear Regression model as well as the Random Forest Regressor. GBR is probably overfitting the training data as well, but not as much as the Random Forest Regressor."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7acb4944",
   "metadata": {},
   "source": [
    "### Compare the three models with MSE, RMSE, and R-squared values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31ad12d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the mse across the folds for all three models\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(linear_scores_table['Fold'], linear_scores_table['MSE'], marker='o', label='Linear Regression', color='blue')\n",
    "plt.plot(rf_scores_table['Fold'], rf_scores_table['MSE'], marker='o', label='Random Forest Regressor', color='orange')\n",
    "plt.plot(gb_scores_table['Fold'], gb_scores_table['MSE'], marker='o', label='Gradient Boosting Regressor', color='green')\n",
    "plt.title('MSE Across Folds for Different Models')\n",
    "plt.xlabel('Fold')\n",
    "plt.ylabel('Mean Squared Error (MSE)')\n",
    "plt.xticks(linear_scores_table['Fold'])\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e345d35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the rmse across the folds for all three models\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(linear_scores_table['Fold'], linear_scores_table['RMSE'], marker='o', label='Linear Regression', color='blue')\n",
    "plt.plot(rf_scores_table['Fold'], rf_scores_table['RMSE'], marker='o', label='Random Forest Regressor', color='orange')\n",
    "plt.plot(gb_scores_table['Fold'], gb_scores_table['RMSE'], marker='o', label='Gradient Boosting Regressor', color='green')\n",
    "plt.title('RMSE Across Folds for Different Models')\n",
    "plt.xlabel('Fold')\n",
    "plt.ylabel('Root Mean Squared Error (RMSE)')\n",
    "plt.xticks(linear_scores_table['Fold'])\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce3d79fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the R^2 across the folds for all three models\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(linear_scores_table['Fold'], linear_scores_table['R^2'], marker='o', label='Linear Regression', color='blue')\n",
    "plt.plot(rf_scores_table['Fold'], rf_scores_table['R^2'], marker='o', label='Random Forest Regressor', color='orange')\n",
    "plt.plot(gb_scores_table['Fold'], gb_scores_table['R^2'], marker='o', label='Gradient Boosting Regressor', color='green')\n",
    "plt.title('R^2 Across Folds for Different Models')\n",
    "plt.xlabel('Fold')\n",
    "plt.ylabel('R-squared')\n",
    "# use log scale for y-axis\n",
    "plt.yscale('log')\n",
    "plt.xticks(linear_scores_table['Fold'])\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ae5c762",
   "metadata": {},
   "source": [
    "| Model | Mean MSE | Mean RMSE | Mean R² | Performance Notes |\n",
    "|-------|----------|-----------|---------|-------------------|\n",
    "| Linear Regression | 3.61M | 1,897 | 1.000000 | Perfect fit due to linear relationship |\n",
    "| Random Forest | 15.54M | 3,939 | 0.999998 | Near-perfect but adds complexity noise |\n",
    "| Gradient Boosting | 424.77M | 20,610 | 0.999949 | Worst performance despite high R² |\n",
    "\n",
    "### Key Insights in comparison of the three models:\n",
    "\n",
    "**Performance Ranking (Best to Worst):**\n",
    "\n",
    "1. **Linear Regression** - Perfect for this linear relationship\n",
    "2. **Random Forest** - 4x higher MSE, slight imperfection \n",
    "3. **Gradient Boosting** - 118x higher MSE, significant overfitting\n",
    "\n",
    "**Why This Happened:**\n",
    "\n",
    "- All models achieve near-perfect R² due to the `squareMeters` perfect correlation\n",
    "- **Linear Regression wins** because the true relationship is perfectly linear\n",
    "- **Tree-based models (RF, GB) add unnecessary complexity** to a simple linear pattern\n",
    "- **Gradient Boosting performs worst** - likely overfitting to training noise with aggressive boosting\n",
    "\n",
    "**Conclusion:**\n",
    "\n",
    "This demonstrates that **model complexity should match data complexity**. For perfectly linear relationships, simple linear regression is optimal. The tree-based models are solving the wrong problem - they're designed for non-linear patterns that don't exist in this dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f94fd0d7",
   "metadata": {},
   "source": [
    "## Feature Importance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5109ab27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of features in training set\n",
    "num_features = X_train.shape[1]\n",
    "print(f\"\\nNumber of features in training set: {num_features}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6f3e02e",
   "metadata": {},
   "source": [
    "We have 23 features in the training set. As seen earlier, the `squareMeters` feature dominates the dataset, and the model is able to explain 100% of the variance in the target variable `price`.\n",
    "\n",
    "However, let's analyze the feature importance to understand the contribution of each feature in the model.\n",
    "\n",
    "We shall use two methods to analyze feature importance:\n",
    "\n",
    "1. Use model-specific feature importance for each of the three models - linear regression, random forest regressor, and gradient boosting regressor.\n",
    "2. Use Permutation Importance to analyze feature importance statistically."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e905f3f9",
   "metadata": {},
   "source": [
    "### Feature Importance from Linear Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3fcd0a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature importance from linear regression model\n",
    "linear_model.fit(X_train, y_train)\n",
    "linear_feature_importance = pd.Series(linear_model.coef_, index=X_train.columns).sort_values(ascending=False)\n",
    "print(\"\\nFeature Importance from Linear Regression Model:\")\n",
    "print(tabulate(linear_feature_importance.reset_index(), headers=['Feature', 'Importance'], tablefmt='grid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebd52e54",
   "metadata": {},
   "source": [
    "As can be seen from the output above, after the `squareMeters` feature, the next 3 most important features are\n",
    "\n",
    "- `hasYard`\n",
    "- `hasPool`\n",
    "- `floors`\n",
    "\n",
    "So, it looks like the cost of the property is also influenced by whether it has a yard or a pool, and the number of floors in the property. But looking at the coefficient value, the importance magnitude is much lower than that of the `squareMeters` feature."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49f6095e",
   "metadata": {},
   "source": [
    "After these, the `cityPartRange_10` seem to have the next highest importance, showing that this part of the city has a significant impact on the property prices, comparted to the other city parts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71868c41",
   "metadata": {},
   "source": [
    "### Feature Importance from Random Forest Regressor Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69c76148",
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature importance for Random Forest Regressor\n",
    "rf_model.fit(X_train, y_train)\n",
    "rf_feature_importance = pd.Series(rf_model.feature_importances_, index=X_train.columns).sort_values(ascending=False)\n",
    "print(\"\\nFeature Importance from Random Forest Regressor:\")\n",
    "print(tabulate(rf_feature_importance.reset_index(), headers=['Feature', 'Importance'], tablefmt='grid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cf3d7c2",
   "metadata": {},
   "source": [
    "Apparently, while `squareMeters` is the most important feature, the next most important features are different in Random Forest Regressor model. `floors` is the only common top 5 feature between Linear Regression and Random Forest Regressor models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a237bb2b",
   "metadata": {},
   "source": [
    "### Feature Importance from Gradient Boosting Regressor Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2dd4ceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature importance for gradient boosting regressor\n",
    "gb_model.fit(X_train, y_train)\n",
    "gb_feature_importance = pd.Series(gb_model.feature_importances_, index=X_train.columns).sort_values(ascending=False)\n",
    "print(\"\\nFeature Importance from Gradient Boosting Regressor:\")\n",
    "print(tabulate(gb_feature_importance.reset_index(), headers=['Feature', 'Importance'], tablefmt='grid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f80d56f",
   "metadata": {},
   "source": [
    "Putting aside the `squareMeters` feature, `hasYard` and `floors` seem to be the next most important top 5 features in the Gradient Boosting Regressor model that is in common with the Linear Regression model.\n",
    "\n",
    "And `floors` is the only common top 5 feature between all three models, apart from the `squareMeters` feature."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7801084f",
   "metadata": {},
   "source": [
    "### Feature Importance using Permutation Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b659e248",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate permutation importance for linear regression model\n",
    "\n",
    "linear_model.fit(X_train, y_train)\n",
    "\n",
    "perm_importance = permutation_importance(\n",
    "    linear_model, \n",
    "    X_test,  # Use test set for unbiased importance\n",
    "    y_test, \n",
    "    n_repeats=10,  # Number of times to permute each feature\n",
    "    random_state=42,\n",
    "    scoring='neg_mean_squared_error'  # Use 'neg_mean_squared_error' for regression\n",
    ")\n",
    "\n",
    "# Get feature names (assuming X_train is a DataFrame)\n",
    "feature_names = X_train.columns if hasattr(X_train, 'columns') else [f'feature_{i}' for i in range(X_train.shape[1])]\n",
    "\n",
    "# Create a DataFrame with results\n",
    "importance_df = pd.DataFrame({\n",
    "    'feature': feature_names,\n",
    "    'importance_mean': perm_importance.importances_mean, # type: ignore\n",
    "    'importance_std': perm_importance.importances_std # type: ignore\n",
    "})\n",
    "\n",
    "# Sort by importance and get top 10\n",
    "top_10_features = importance_df.sort_values('importance_mean', ascending=False).head(10)\n",
    "\n",
    "print(\"Top 10 Features by Permutation Importance:\")\n",
    "print(top_10_features)\n",
    "\n",
    "# Plot the results\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(range(10), top_10_features['importance_mean'][::-1])\n",
    "plt.yticks(range(10), top_10_features['feature'][::-1]) # type: ignore\n",
    "plt.xlabel('Permutation Importance')\n",
    "plt.title('Top 10 Features by Permutation Importance')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dba839d7",
   "metadata": {},
   "source": [
    "Based on the Permutation Importance analysis, we can see that the `squareMeters` feature is by far the most important feature, followed by `floors`, `hasYard`, and `hasPool`.\n",
    "\n",
    "Other than the primary feature `squareMeters`, the order of other features importance is different compared to the feature importance method used for linear regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bec55b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate permutation importance for random forest regressor model\n",
    "rf_model.fit(X_train, y_train)  \n",
    "\n",
    "perm_importance_rf = permutation_importance(\n",
    "    rf_model, \n",
    "    X_test,  # Use test set for unbiased importance\n",
    "    y_test, \n",
    "    n_repeats=10,  # Number of times to permute each feature\n",
    "    random_state=42,\n",
    "    scoring='neg_mean_squared_error'  # Use 'neg_mean_squared_error' for regression\n",
    ")\n",
    "\n",
    "# Get feature names (assuming X_train is a DataFrame)\n",
    "feature_names_rf = X_train.columns if hasattr(X_train, 'columns') else [f'feature_{i}' for i in range(X_train.shape[1])]        \n",
    "\n",
    "# Create a DataFrame with results\n",
    "importance_df_rf = pd.DataFrame({\n",
    "    'feature': feature_names_rf,\n",
    "    'importance_mean': perm_importance_rf.importances_mean, # type: ignore\n",
    "    'importance_std': perm_importance_rf.importances_std # type: ignore\n",
    "})\n",
    "\n",
    "# Sort by importance and get top 10\n",
    "top_10_features_rf = importance_df_rf.sort_values('importance_mean', ascending=False).head(10)  \n",
    "\n",
    "print(\"Top 10 Features by Permutation Importance (Random Forest):\")\n",
    "print(top_10_features_rf)\n",
    "\n",
    "# Plot the results for Random Forest\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(range(10), top_10_features_rf['importance_mean'][::-1])\n",
    "plt.yticks(range(10), top_10_features_rf['feature'][::-1]) # type: ignore\n",
    "plt.xlabel('Permutation Importance')\n",
    "plt.title('Top 10 Features by Permutation Importance (Random Forest)')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06b42d2f",
   "metadata": {},
   "source": [
    "The top 4 features identified by Permutation Importance are same as those identified for the Linear regression model. And these are\n",
    "\n",
    "- `squareMeters`\n",
    "- `floors`\n",
    "- `hasYard`\n",
    "- `hasPool`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0590375",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate permutation importance for gradient boosting regressor model\n",
    "gb_model.fit(X_train, y_train)\n",
    "\n",
    "perm_importance_gb = permutation_importance(\n",
    "    gb_model, \n",
    "    X_test,  # Use test set for unbiased importance\n",
    "    y_test, \n",
    "    n_repeats=10,  # Number of times to permute each feature\n",
    "    random_state=42,\n",
    "    scoring='neg_mean_squared_error'  # Use 'neg_mean_squared_error' for regression\n",
    ")\n",
    "\n",
    "# Get feature names (assuming X_train is a DataFrame)\n",
    "feature_names_gb = X_train.columns if hasattr(X_train, 'columns') else [f'feature_{i}' for i in range(X_train.shape[1])]    \n",
    "\n",
    "# Create a DataFrame with results\n",
    "importance_df_gb = pd.DataFrame({\n",
    "    'feature': feature_names_gb,\n",
    "    'importance_mean': perm_importance_gb.importances_mean, # type: ignore\n",
    "    'importance_std': perm_importance_gb.importances_std # type: ignore\n",
    "})\n",
    "\n",
    "# Sort by importance and get top 10\n",
    "top_10_features_gb = importance_df_gb.sort_values('importance_mean', ascending=False).head(10)  \n",
    "print(\"Top 10 Features by Permutation Importance (Gradient Boosting):\")\n",
    "print(top_10_features_gb)\n",
    "\n",
    "# Plot the results for Gradient Boosting\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(range(10), top_10_features_gb['importance_mean'][::-1])\n",
    "plt.yticks(range(10), top_10_features_gb['feature'][::-1]) # type: ignore\n",
    "plt.xlabel('Permutation Importance')\n",
    "plt.title('Top 10 Features by Permutation Importance (Gradient Boosting)')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff5aa302",
   "metadata": {},
   "source": [
    "And with the gradient boosting regressor model, the top 3 are common between all three models:\n",
    "\n",
    "- `squareMeters`\n",
    "- `floors`\n",
    "- `hasYard`\n",
    "\n",
    "Putting all the results together, it appears that the size of the property in square meters along with the number of floors and whether it has a yard or a pool are the most important features that influence the property prices in this dataset.\n",
    "\n",
    "The location of the property, represented by `cityPartRange`, also plays a significant role in determining the price, but it is not as significant as the size and features of the property itself."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80312edc",
   "metadata": {},
   "source": [
    "### Feature Importance using SHAP Values\n",
    "\n",
    "Let us now analyze the feature importance using SHAP values. SHAP values provide a unified measure of feature importance and can help us understand the contribution of each feature to the model's predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6df72fcb",
   "metadata": {},
   "source": [
    "#### SHAP Values for Linear Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28f57262",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SHAP explainer expects numerical data. Convert category columns to their codes.\n",
    "X_train_shap = X_train.copy()\n",
    "X_test_shap = X_test.copy()\n",
    "\n",
    "for col in X_train_shap.select_dtypes(include='category').columns:\n",
    "\tX_train_shap[col] = X_train_shap[col].cat.codes\n",
    "\tX_test_shap[col] = X_test_shap[col].cat.codes\n",
    "\n",
    "# Convert boolean columns to integers\n",
    "for col in X_train_shap.select_dtypes(include='bool').columns:\n",
    "\tX_train_shap[col] = X_train_shap[col].astype(int)\n",
    "\tX_test_shap[col] = X_test_shap[col].astype(int)\n",
    "\n",
    "# Create SHAP explainer\n",
    "explainer = shap.Explainer(linear_model, X_train_shap)\n",
    "\n",
    "# Calculate SHAP values\n",
    "shap_values = explainer(X_test_shap)\n",
    "\n",
    "# Plot SHAP values\n",
    "shap.summary_plot(shap_values, X_test_shap)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91031a5c",
   "metadata": {},
   "source": [
    "The SHAP values order seem to be same for the first 5 features as the feature importance from the Linear Regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f567252",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SHAP dependence plot for squareMeters\n",
    "shap.dependence_plot('squareMeters', shap_values.values, X_test_shap,\n",
    "                     interaction_index=None, show=False)\n",
    "plt.title('SHAP Dependence Plot for squareMeters')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca32439a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SHAP Dependence plot for hasYard\n",
    "shap.dependence_plot('hasYard', shap_values.values, X_test_shap,\n",
    "                     interaction_index=None, show=False)\n",
    "plt.title('SHAP Dependence Plot for hasYard')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa86baf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SHAP Dependence plot for floors\n",
    "shap.dependence_plot('floors', shap_values.values, X_test_shap,\n",
    "                     interaction_index=None, show=False)\n",
    "plt.title('SHAP Dependence Plot for floors')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f33868c",
   "metadata": {},
   "source": [
    "**Analysis of SHAP Dependence Plots:**\n",
    "\n",
    "- Strong positive impact of `squareMeters` on price, as expected\n",
    "- `floors` too shows a clear positive relationship with price, indicating that more floors generally lead to higher prices, but the strength is lower\n",
    "- `hasYard` shows a clear positive relationship with price, indicating that properties with yards tend to have higher prices\n",
    "\n",
    "We only analyzed three of top 5 features, in order to compare with the SHAP values from Random Forest Regressor model.\n",
    "\n",
    "Since `hasPool` is also binary in nature, we omitted it from dependency plot analysis, as it would show similar results to `hasYard`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa7d1ac9",
   "metadata": {},
   "source": [
    "#### SHAP Values for Random Forest Regressor Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fd3f15c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SHAP with Random Forest Regressor\n",
    "explainer_rf = shap.Explainer(rf_model, X_train_shap)\n",
    "shap_values_rf = explainer_rf(X_test_shap)\n",
    "\n",
    "# Plot SHAP values for Random Forest Regressor\n",
    "shap.summary_plot(shap_values_rf, X_test_shap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e38f14cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SHAP dependence plot for squareMeters\n",
    "shap.dependence_plot('squareMeters', shap_values_rf.values, X_test_shap,\n",
    "                     interaction_index=None, show=False)\n",
    "plt.title('SHAP Dependence Plot for squareMeters')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f2d705b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SHAP dependence plot for floors\n",
    "shap.dependence_plot('floors', shap_values_rf.values, X_test_shap,\n",
    "                     interaction_index=None, show=False)\n",
    "plt.title('SHAP Dependence Plot for floors')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "819f0f94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SHAP dependence for hasYard\n",
    "shap.dependence_plot('hasYard', shap_values_rf.values, X_test_shap,\n",
    "                     interaction_index=None, show=False)\n",
    "plt.title('SHAP Dependence Plot for hasYard')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f8590bb",
   "metadata": {},
   "source": [
    "**Analysis of SHAP Dependence Plots:**\n",
    "\n",
    "- Strong positive impact of `squareMeters` on price, as expected\n",
    "- The impact of `floors` is also positive, but since the Random Forest is a non-linear model, the relationship is not as clear as in the Linear Regression model.\n",
    "- And `hasYard` while showing a postive relationship, it does have significant noise in the SHAP values, indicating that the model is not able to explain the relationship between `hasYard` and `price` as well as it does for `squareMeters` and `floors`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0642ed5",
   "metadata": {},
   "source": [
    "### SHAP Values for Gradient Boosting Regressor Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4298a717",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SHAP values for Gradient Boosting Regressor Model\n",
    "explainer_gb = shap.Explainer(gb_model, X_train_shap)\n",
    "shap_values_gb = explainer_gb(X_test_shap)  \n",
    "\n",
    "# SHAP values for Gradient Boosting Regressor Model\n",
    "shap.summary_plot(shap_values_gb, X_test_shap)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "739c09bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# shap dependence plot for squareMeters\n",
    "shap.dependence_plot('squareMeters', shap_values_gb.values, X_test_shap,\n",
    "                     interaction_index=None, show=False)\n",
    "plt.title('SHAP Dependence Plot for squareMeters')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f104f37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SHAP dependence plot for floors\n",
    "shap.dependence_plot('floors', shap_values_gb.values, X_test_shap,\n",
    "                     interaction_index=None, show=False)\n",
    "plt.title('SHAP Dependence Plot for floors')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3a82e0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SHAP dependence plot for hasYard\n",
    "shap.dependence_plot('hasYard', shap_values_gb.values, X_test_shap,\n",
    "                     interaction_index=None, show=False)\n",
    "plt.title('SHAP Dependence Plot for hasYard')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0d12213",
   "metadata": {},
   "source": [
    "**Analysis of SHAP Dependence Plots:**\n",
    "\n",
    "- Strong positive impact of `squareMeters` on price, as expected\n",
    "- `floors` has a positive impact, but less pronounced than `squareMeters`. There is some kind of steps noticed in the SHAP values, indicating that if the number of floors is higher than the mean floors, the price increases significantly.\n",
    "- `hasYard` has a slightly better relationship with price with Gradient Boosting Regressor model compared to Random Forest model, as noticed from lesser overlap of the SHAP values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e98c2544",
   "metadata": {},
   "source": [
    "## Final Model Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0f9f52f",
   "metadata": {},
   "source": [
    "We shall use the Linear Regression model as the final model for this task, as it is the simplest model that is able to explain 100% of the variance in the target variable `price`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc19ee87",
   "metadata": {},
   "source": [
    "As we saw earlier, top 5 features of the linear regression model are more than sufficient to explain the variance in the target variable `price`, and the model is able to predict the price of the properties with a very high accuracy.\n",
    "\n",
    "The top 5 features are:\n",
    "- `squareMeters`\n",
    "- `floors`\n",
    "- `hasYard`\n",
    "- `hasPool`\n",
    "- `cityPartRange_10`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a94f748",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select the top 5 features from the original dataset\n",
    "top_5_features = ['squareMeters', 'floors', 'hasYard', 'hasPool', 'cityPartRange']\n",
    "\n",
    "paris_real_estate_top5 = paris_real_estate_processing[top_5_features + ['price']]\n",
    "\n",
    "# split the top 5 features dataset into train and test sets\n",
    "X_train_top5, X_test_top5, y_train_top5, y_test_top5 = train_test_split(\n",
    "    paris_real_estate_top5.drop(columns=['price']),\n",
    "    paris_real_estate_top5['price'],\n",
    "    test_size=0.2,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# standardize the features that need to be standardized\n",
    "scaler_top5 = StandardScaler()\n",
    "X_train_top5[['squareMeters', 'floors']] = scaler_top5.fit_transform(X_train_top5[['squareMeters', 'floors']])\n",
    "X_test_top5[['squareMeters', 'floors']] = scaler_top5.transform(X_test_top5[['squareMeters', 'floors']])\n",
    "\n",
    "# transform cityPartRange to 0 or 1 based on the if the cityPartRange is 10 or not\n",
    "X_train_top5['cityPartRange_10'] = (X_train_top5['cityPartRange'] == 10).astype(int)\n",
    "X_test_top5['cityPartRange_10'] = (X_test_top5['cityPartRange'] == 10).astype(int)\n",
    "\n",
    "# drop the original cityPartRange column\n",
    "X_train_top5 = X_train_top5.drop(columns=['cityPartRange'])\n",
    "X_test_top5 = X_test_top5.drop(columns=['cityPartRange'])\n",
    "\n",
    "# print the shape of the top 5 features train and test sets\n",
    "print(f\"\\nShape of the training set with top 5 features: {X_train_top5.shape}\")\n",
    "print(f\"Shape of the testing set with top 5 features: {X_test_top5.shape}\")\n",
    "\n",
    "# save the processed top 5 features train and test sets to CSV files\n",
    "X_train_top5.to_csv('X_train_top5_processed.csv', index=False)\n",
    "X_test_top5.to_csv('X_test_top5_processed.csv', index=False)\n",
    "y_train_top5.to_csv('y_train_top5_processed.csv', index=False)\n",
    "y_test_top5.to_csv('y_test_top5_processed.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "251769a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the linear regression model on the entire training set\n",
    "linear_model_top5 = LinearRegression()\n",
    "linear_model_top5.fit(X_train_top5, y_train_top5)\n",
    "\n",
    "# print the coefficients of the linear regression model\n",
    "linear_coefficients = pd.Series(linear_model_top5.coef_, index=X_train_top5.columns).sort_values(ascending=False)\n",
    "print(\"\\nLinear Regression Model Coefficients for Top 5 Features:\")\n",
    "print(tabulate(linear_coefficients.reset_index(), headers=['Feature', 'Coefficient'], tablefmt='grid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bd9b18e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the coefficients of original linear regression model for top 5 features\n",
    "original_linear_coefficients = pd.Series(linear_model.coef_, index=X_train.columns).sort_values(ascending=False)\n",
    "print(\"\\nOriginal Linear Regression Model Coefficients for Top 5 Features:\")\n",
    "print(tabulate(original_linear_coefficients.head(5).reset_index(), headers=['Feature', 'Coefficient'], tablefmt='grid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba5673c8",
   "metadata": {},
   "source": [
    "The coefficients are pretty similar between the original linear regression model and the model trained on the top 5 features, indicating that the top 5 features are sufficient to explain the variance in the target variable `price`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eaacb9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluation metrics for the top5 linear regression model\n",
    "y_pred_top5 = linear_model_top5.predict(X_test_top5)\n",
    "mse_top5 = mean_squared_error(y_test_top5, y_pred_top5)\n",
    "rmse_top5 = mse_top5 ** 0.5\n",
    "r2_top5 = r2_score(y_test_top5, y_pred_top5)  \n",
    "\n",
    "# print the evaluation metrics\n",
    "print(f\"\\nEvaluation Metrics for Top 5 Linear Regression Model:\")\n",
    "print(f\"Mean Squared Error (MSE): {mse_top5:.2f}\")\n",
    "print(f\"Root Mean Squared Error (RMSE): {rmse_top5:.2f}\")\n",
    "print(f\"R-squared (R^2): {r2_top5:.2f}\")        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0953dd41",
   "metadata": {},
   "source": [
    "Let us now create a wrapper function that will take just one input and predict the price of the property."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "386de8bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to predict the price of a property based on top 5 features\n",
    "def predict_price_top5(square_meters, floors, has_yard, has_pool, city_part_range):\n",
    "    \"\"\"\n",
    "    Predict the price of a property based on top 5 features.\n",
    "    \n",
    "    Parameters:\n",
    "    square_meters (float): Size of the property in square meters.\n",
    "    floors (int): Number of floors in the property.\n",
    "    has_yard (bool): Whether the property has a yard or not.\n",
    "    has_pool (bool): Whether the property has a pool or not.\n",
    "    city_part_range (int): City part range (e.g., 10).\n",
    "    \n",
    "    Returns:\n",
    "    float: Predicted price of the property.\n",
    "    \"\"\"\n",
    "    # create a DataFrame with the input features\n",
    "    input_data = pd.DataFrame({\n",
    "        'squareMeters': [square_meters],\n",
    "        'floors': [floors],\n",
    "        'hasYard': [1 if has_yard else 0],\n",
    "        'hasPool': [1 if has_pool else 0],\n",
    "        'cityPartRange_10': [1 if city_part_range == 10 else 0]\n",
    "    })\n",
    "    \n",
    "    # standardize the features\n",
    "    input_data[['squareMeters', 'floors']] = scaler_top5.transform(input_data[['squareMeters', 'floors']])\n",
    "    \n",
    "    # predict the price\n",
    "    predicted_price = linear_model_top5.predict(input_data)\n",
    "    \n",
    "    return predicted_price[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bba37d18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample input for prediction\n",
    "# this is very close to the second sample in the dataset\n",
    "sample_square_meters = 80000\n",
    "sample_floors = 90\n",
    "sample_has_yard = True\n",
    "sample_has_pool = True\n",
    "sample_city_part_range = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4b38b22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict the price of the property\n",
    "predicted_price = predict_price_top5(sample_square_meters, sample_floors, sample_has_yard, sample_has_pool, sample_city_part_range)\n",
    "print(f\"\\nPredicted price for the property is: {predicted_price:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "479ba4a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# actual price for the second sample in the dataset\n",
    "actual_price = paris_real_estate_top5.iloc[1]['price']\n",
    "print(f\"\\nActual price for the property is: {actual_price:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5ca3ff8",
   "metadata": {},
   "source": [
    "The output predicted based on just 5 top features with a linear regression model is very close to the actual price for the second sample in the dataset, indicating that the model is able to predict the price of the property with a very high accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1397ace7",
   "metadata": {},
   "source": [
    "We shall need the model and the scaler to predict the price of the property. We will save both the model and the scaler to files so that we can use them later for prediction without needing to retrain the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d43f925",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model to a file\n",
    "joblib.dump(linear_model_top5, 'linear_model_top5.pkl')\n",
    "print(\"\\nLinear Regression Model for Top 5 Features saved to 'linear_model_top5.pkl'.\")\n",
    "\n",
    "# save the scaler to a file\n",
    "joblib.dump(scaler_top5, 'scaler_top5.pkl')\n",
    "print(\"Scaler for Top 5 Features saved to 'scaler_top5.pkl'.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "task5d",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
